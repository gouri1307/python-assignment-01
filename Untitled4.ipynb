{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gouri1307/python-assignment-01/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpneurjF-jWM"
      },
      "source": [
        "Part I: Process Automation\n",
        "Q1. Create a file that contains 1000 lines of random strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrWNbrPi-wN_",
        "outputId": "edb21259-f60c-46b1-be90-0ef8bcb61f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A file with 1000 lines of random strings has been created: random_strings.txt\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def generate_random_string(length):\n",
        "    \"\"\"Generates a random string of given length.\"\"\"\n",
        "    letters = string.ascii_lowercase\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "# Generate random strings and write them to a file\n",
        "file_path = 'random_strings.txt'  # Path to the output file\n",
        "num_lines = 1000  # Number of lines to generate\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    for _ in range(num_lines):\n",
        "        random_string = generate_random_string(10)  # Change 10 to adjust the length of each string\n",
        "        file.write(random_string + '\\n')\n",
        "\n",
        "print(f\"A file with {num_lines} lines of random strings has been created: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XzMkrPZ-9eX"
      },
      "source": [
        " Q2. Create a file that contains multiple lines of random strings and file size must be 5 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXLgCXiH_HAK",
        "outputId": "95a2cfad-8e26-4eaf-89a5-790c046077b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A file with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings.txt\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "def generate_random_string(length):\n",
        "    \"\"\"Generates a random string of given length.\"\"\"\n",
        "    letters = string.ascii_lowercase\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "# Generate random strings and write them to a file\n",
        "file_path = 'random_strings.txt'  # Path to the output file\n",
        "target_file_size = 5 * 1024 * 1024  # 5 MB\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    file_size = 0\n",
        "    while file_size < target_file_size:\n",
        "        random_string = generate_random_string(random.randint(10, 100))  # Varying length between 10 and 100\n",
        "        file.write(random_string + '\\n')\n",
        "        file_size = os.path.getsize(file_path)\n",
        "\n",
        "print(f\"A file with multiple lines of random strings, approximately 5 MB in size, has been created: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34P22_yt_KfI"
      },
      "source": [
        "Q3. Create 10 files that contains multiple lines of random strings and file size of each file must be 5 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zifd3odu_SkA",
        "outputId": "73685aaa-a021-4b26-bd64-ece18e18bf9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 1 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_1.txt\n",
            "File 2 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_2.txt\n",
            "File 3 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_3.txt\n",
            "File 4 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_4.txt\n",
            "File 5 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_5.txt\n",
            "File 6 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_6.txt\n",
            "File 7 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_7.txt\n",
            "File 8 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_8.txt\n",
            "File 9 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_9.txt\n",
            "File 10 with multiple lines of random strings, approximately 5 MB in size, has been created: random_strings_10.txt\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "def generate_random_string(length):\n",
        "    \"\"\"Generates a random string of given length.\"\"\"\n",
        "    letters = string.ascii_lowercase\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "# Generate 10 files with multiple lines of random strings, each approximately 5 MB in size\n",
        "num_files = 10\n",
        "target_file_size = 5 * 1024 * 1024  # 5 MB\n",
        "\n",
        "for i in range(num_files):\n",
        "    file_path = f'random_strings_{i+1}.txt'  # Path to the output file\n",
        "    with open(file_path, 'w') as file:\n",
        "        file_size = 0\n",
        "        while file_size < target_file_size:\n",
        "            random_string = generate_random_string(random.randint(10, 100))  # Varying length between 10 and 100\n",
        "            file.write(random_string + '\\n')\n",
        "            file_size = os.path.getsize(file_path)\n",
        "\n",
        "    print(f\"File {i+1} with multiple lines of random strings, approximately 5 MB in size, has been created: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP2NLH3y_aP5"
      },
      "source": [
        "Q4. Create 5 files of size 1GB, 2GB, 3GB, 4GB and 5GB; file contains multiple lines of random strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j1gggTgZAD4X",
        "outputId": "28bd1220-9b2c-4b64-8832-38a1b30ae03d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File of size 1GB with multiple lines of random strings has been created: random_strings_1GB.txt\n",
            "File of size 2GB with multiple lines of random strings has been created: random_strings_2GB.txt\n",
            "File of size 3GB with multiple lines of random strings has been created: random_strings_3GB.txt\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "def generate_random_string(length):\n",
        "    \"\"\"Generates a random string of given length.\"\"\"\n",
        "    letters = string.ascii_lowercase\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "# Generate files of different sizes, each containing multiple lines of random strings\n",
        "file_sizes = [1, 2, 3, 4, 5]  # In GB\n",
        "\n",
        "for size in file_sizes:\n",
        "    file_path = f'random_strings_{size}GB.txt'  # Path to the output file\n",
        "    target_file_size = size * 1024 * 1024 * 1024  # Convert GB to bytes\n",
        "    chunk_size = 1024 * 1024  # 1 MB chunk size\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        file_size = 0\n",
        "        while file_size < target_file_size:\n",
        "            random_string = generate_random_string(random.randint(10, 100))  # Varying length between 10 and 100\n",
        "            file.write(random_string + '\\n')\n",
        "            file_size = os.path.getsize(file_path)\n",
        "\n",
        "            # Flush the file buffer in chunks to minimize memory usage\n",
        "            if file_size % chunk_size == 0:\n",
        "                file.flush()\n",
        "\n",
        "    print(f\"File of size {size}GB with multiple lines of random strings has been created: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhbHCGXvAEvq"
      },
      "source": [
        "Q5. Convert all the files of Q4 into upper case one by one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5znH4KqAM7h"
      },
      "outputs": [],
      "source": [
        "# Convert the contents of the files to uppercase\n",
        "for size in file_sizes:\n",
        "    file_path = f'random_strings_{size}GB.txt'  # Path to the input file\n",
        "    output_file_path = f'random_strings_{size}GB_uppercase.txt'  # Path to the output file\n",
        "\n",
        "    with open(file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
        "        for line in input_file:\n",
        "            converted_line = line.upper()\n",
        "            output_file.write(converted_line)\n",
        "\n",
        "    print(f\"File {file_path} converted to uppercase and saved as {output_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKuTDf5mAPxZ"
      },
      "source": [
        "Q6. Convert all the files of Q4 into upper case parallel using multi-threading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOURZRvuAefL"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "\n",
        "# Function to convert file contents to uppercase\n",
        "def convert_to_uppercase(file_path):\n",
        "    output_file_path = f'{file_path}_uppercase'  # Path to the output file\n",
        "\n",
        "    with open(file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
        "        for line in input_file:\n",
        "            converted_line = line.upper()\n",
        "            output_file.write(converted_line)\n",
        "\n",
        "    return output_file_path\n",
        "\n",
        "# Convert the contents of the files to uppercase using multi-threading\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Submit conversion tasks for each file\n",
        "    conversion_tasks = [executor.submit(convert_to_uppercase, f'random_strings_{size}GB.txt') for size in file_sizes]\n",
        "\n",
        "    # Retrieve the converted file paths as they complete\n",
        "    converted_files = [task.result() for task in concurrent.futures.as_completed(conversion_tasks)]\n",
        "\n",
        "# Print the paths of the converted files\n",
        "for file_path in converted_files:\n",
        "    print(f\"File {file_path} converted to uppercase.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvKeS7EkAfl3"
      },
      "source": [
        "Q7. WAP to automatically download 10 images of cat from “Google Images”. [Hint: Find the package from\n",
        "pypi.org and use it]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etRpLWanAj8z"
      },
      "outputs": [],
      "source": [
        "import google_images_download\n",
        "\n",
        "# Specify search query and download parameters\n",
        "search_query = 'cat'\n",
        "num_images = 10\n",
        "\n",
        "# Create an instance of the downloader\n",
        "downloader = google_images_download.googleimagesdownload()\n",
        "\n",
        "# Define the download arguments\n",
        "download_arguments = {\n",
        "    'keywords': search_query,\n",
        "    'limit': num_images,\n",
        "    'print_urls': False\n",
        "}\n",
        "\n",
        "# Perform the image search and download\n",
        "results = downloader.download(download_arguments)\n",
        "\n",
        "# Print the downloaded images' paths\n",
        "for query, paths in results.items():\n",
        "    for path in paths:\n",
        "        print(f\"Downloaded image: {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZjgNG6zA2lq"
      },
      "source": [
        "Q8. WAP to automatically download 10 videos of “Machine Learning” from “Youtube.com”. [Hint: Find the\n",
        "package from pypi.org and use it]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLx-Y46IA-bt"
      },
      "outputs": [],
      "source": [
        "import pytube\n",
        "\n",
        "# Specify search query and download parameters\n",
        "search_query = 'Machine Learning'\n",
        "num_videos = 10\n",
        "\n",
        "# Perform the YouTube search\n",
        "search_results = pytube.Search(search_query)\n",
        "\n",
        "# Iterate over the search results and download the videos\n",
        "downloaded_videos = 0\n",
        "\n",
        "for video in search_results.results:\n",
        "    try:\n",
        "        video_url = f\"https://www.youtube.com/watch?v={video.video_id}\"\n",
        "        yt = pytube.YouTube(video_url)\n",
        "        stream = yt.streams.get_highest_resolution()\n",
        "\n",
        "        # Download the video\n",
        "        stream.download()\n",
        "        downloaded_videos += 1\n",
        "\n",
        "        print(f\"Downloaded video {downloaded_videos}: {yt.title}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading video: {video.video_id}. {str(e)}\")\n",
        "\n",
        "    if downloaded_videos == num_videos:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LufCZY43BI77"
      },
      "source": [
        "Q9. Convert all the videos of Q8 and convert it to audio. [Hint: Find the package from pypi.org and use it]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suNN5-S6BPXk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from moviepy.editor import *\n",
        "\n",
        "# Convert videos to audio\n",
        "for i in range(1, num_videos+1):\n",
        "    video_file_path = f\"video{i}.mp4\"\n",
        "    audio_file_path = f\"audio{i}.mp3\"\n",
        "\n",
        "    # Check if the video file exists\n",
        "    if os.path.isfile(video_file_path):\n",
        "        # Load the video file\n",
        "        video = VideoFileClip(video_file_path)\n",
        "\n",
        "        # Extract the audio and save as an MP3 file\n",
        "        audio = video.audio\n",
        "        audio.write_audiofile(audio_file_path)\n",
        "\n",
        "        # Close the video and audio clips\n",
        "        video.close()\n",
        "        audio.close()\n",
        "\n",
        "        print(f\"Video {video_file_path} converted to audio: {audio_file_path}\")\n",
        "    else:\n",
        "        print(f\"Video file {video_file_path} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCMNhpRkBXyG"
      },
      "source": [
        "Q10. Create an automated pipeline using multi-threading for:\n",
        "“Automatic Download of 100 Videos from YouTube” → “Convert it to Audio”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ByDHfhhBang"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import concurrent.futures\n",
        "import pytube\n",
        "from moviepy.editor import *\n",
        "\n",
        "# Specify search query and download parameters\n",
        "search_query = 'Machine Learning'\n",
        "num_videos = 100\n",
        "\n",
        "# Perform the YouTube search\n",
        "search_results = pytube.Search(search_query)\n",
        "\n",
        "# Create a directory to store the downloaded videos\n",
        "if not os.path.exists(\"videos\"):\n",
        "    os.makedirs(\"videos\")\n",
        "\n",
        "# Function to download a video\n",
        "def download_video(video):\n",
        "    try:\n",
        "        video_url = f\"https://www.youtube.com/watch?v={video.video_id}\"\n",
        "        yt = pytube.YouTube(video_url)\n",
        "        stream = yt.streams.get_highest_resolution()\n",
        "\n",
        "        # Download the video\n",
        "        video_path = os.path.join(\"videos\", f\"{video.video_id}.mp4\")\n",
        "        stream.download(output_path=\"videos\", filename=video.video_id)\n",
        "\n",
        "        return video_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading video: {video.video_id}. {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Function to convert video to audio\n",
        "def convert_to_audio(video_path):\n",
        "    try:\n",
        "        audio_path = video_path.replace(\".mp4\", \".mp3\")\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        audio.write_audiofile(audio_path)\n",
        "        video.close()\n",
        "        audio.close()\n",
        "\n",
        "        return audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting video to audio: {video_path}. {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Perform the download and conversion using multi-threading\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Download videos\n",
        "    download_tasks = [executor.submit(download_video, video) for video in search_results.results[:num_videos]]\n",
        "    downloaded_videos = [task.result() for task in concurrent.futures.as_completed(download_tasks) if task.result() is not None]\n",
        "\n",
        "    # Convert videos to audio\n",
        "    convert_tasks = [executor.submit(convert_to_audio, video_path) for video_path in downloaded_videos]\n",
        "    converted_audios = [task.result() for task in concurrent.futures.as_completed(convert_tasks) if task.result() is not None]\n",
        "\n",
        "# Print the paths of the converted audios\n",
        "for audio_path in converted_audios:\n",
        "    print(f\"Converted audio: {audio_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdnJE_KDB0nb"
      },
      "source": [
        "Q11. Create an automated pipeline using multi-threading for: “Automatic Download of 500 images of Dog from\n",
        "GoogleImages” → “Rescale it to 50%”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRwa14TEB3tF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import concurrent.futures\n",
        "import requests\n",
        "from google_images_download import google_images_download\n",
        "from PIL import Image\n",
        "\n",
        "# Specify search query and download parameters\n",
        "search_query = 'dog'\n",
        "num_images = 500\n",
        "\n",
        "# Create a directory to store the downloaded images\n",
        "if not os.path.exists(\"images\"):\n",
        "    os.makedirs(\"images\")\n",
        "\n",
        "# Create an instance of the downloader\n",
        "downloader = google_images_download.googleimagesdownload()\n",
        "\n",
        "# Define the download arguments\n",
        "download_arguments = {\n",
        "    'keywords': search_query,\n",
        "    'limit': num_images,\n",
        "    'print_urls': False,\n",
        "    'output_directory': 'images'\n",
        "}\n",
        "\n",
        "# Perform the image search and download\n",
        "results = downloader.download(download_arguments)\n",
        "\n",
        "# Function to rescale an image\n",
        "def rescale_image(image_path):\n",
        "    try:\n",
        "        # Open the image\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Calculate the new dimensions for rescaling\n",
        "        new_width = int(image.width * 0.5)\n",
        "        new_height = int(image.height * 0.5)\n",
        "\n",
        "        # Rescale the image\n",
        "        resized_image = image.resize((new_width, new_height))\n",
        "\n",
        "        # Save the rescaled image\n",
        "        resized_image_path = image_path.replace(\".jpg\", \"_rescaled.jpg\")\n",
        "        resized_image.save(resized_image_path)\n",
        "\n",
        "        return resized_image_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error rescaling image: {image_path}. {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Perform the rescaling using multi-threading\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Get the paths of the downloaded images\n",
        "    image_paths = [os.path.join(\"images\", file) for file in os.listdir(\"images\") if file.endswith(\".jpg\")]\n",
        "\n",
        "    # Submit rescaling tasks for each image\n",
        "    rescale_tasks = [executor.submit(rescale_image, image_path) for image_path in image_paths]\n",
        "    rescaled_images = [task.result() for task in concurrent.futures.as_completed(rescale_tasks) if task.result() is not None]\n",
        "\n",
        "# Print the paths of the rescaled images\n",
        "for image_path in rescaled_images:\n",
        "    print(f\"Rescaled image: {image_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcB4i0yqcyAD"
      },
      "source": [
        "Part II: Data Analytics\n",
        "Q12. Create a random dataset of 100 rows and 30 columns. All the values are defined between [1,200]. Perform\n",
        "the following operations:\n",
        "(i) Replace all the values with NA in the dataset defined between [10, 60]. Print the count of number\n",
        "rows having missing values.\n",
        "(ii) Replace all the NA values with the average of the column value.\n",
        "(iii) Find the Pearson correlation among all the columns and plot heat map. Also select those columns\n",
        "having correlation <=0.7.\n",
        "(iv) Normalize all the values in the dataset between 0 and 10.\n",
        "(v) Replace all the values in the dataset with 1 if value <=0.5 else with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg_5CWJedL2g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a random dataset\n",
        "dataset = pd.DataFrame(np.random.randint(1, 201, size=(100, 30)), columns=[f'col_{i}' for i in range(30)])\n",
        "\n",
        "# (i) Replace values with NA in the defined range [10, 60]\n",
        "dataset.loc[10:60] = np.nan\n",
        "\n",
        "# Count the number of rows having missing values\n",
        "missing_rows_count = dataset.isnull().sum(axis=1).sum()\n",
        "print(\"Number of rows with missing values:\", missing_rows_count)\n",
        "\n",
        "# (ii) Replace NA values with the average of the column\n",
        "dataset = dataset.fillna(dataset.mean())\n",
        "\n",
        "# (iii) Calculate Pearson correlation and plot heat map\n",
        "correlation_matrix = dataset.corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Pearson Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# Select columns having correlation <= 0.7\n",
        "correlation_threshold = 0.7\n",
        "columns_with_low_correlation = correlation_matrix.columns[correlation_matrix.max() <= correlation_threshold]\n",
        "print(\"Columns with correlation <= 0.7:\", columns_with_low_correlation.tolist())\n",
        "\n",
        "# (iv) Normalize values between 0 and 10\n",
        "dataset_normalized = (dataset - dataset.min()) / (dataset.max() - dataset.min()) * 10\n",
        "\n",
        "# (v) Replace values with 1 if <= 0.5 else with 0\n",
        "dataset_binary = dataset_normalized.applymap(lambda x: 1 if x <= 0.5 else 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcO7oALGdPRj"
      },
      "source": [
        "Q13. Create a random dataset of 500 rows and 10 columns.\n",
        "Columns 1 to 4 are defined between [-10, 10];\n",
        "Columns 5 to 8 are defined between [10, 20];\n",
        "Columns 9 to 10 are defined between [-100, 100].\n",
        "Apply following clustering algorithms; determine the optimal number of clusters and plot distance metric\n",
        "graph using each algorithm.\n",
        "(i) K-Mean clustering\n",
        "(ii) Hierarchical clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82fAiiKudSpI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a random dataset\n",
        "dataset = pd.DataFrame(np.random.randint(1, 201, size=(100, 30)), columns=[f'col_{i}' for i in range(30)])\n",
        "\n",
        "# (i) Replace values with NA in the defined range [10, 60]\n",
        "dataset.loc[10:60] = np.nan\n",
        "\n",
        "# Count the number of rows having missing values\n",
        "missing_rows_count = dataset.isnull().sum(axis=1).sum()\n",
        "print(\"Number of rows with missing values:\", missing_rows_count)\n",
        "\n",
        "# (ii) Replace NA values with the average of the column\n",
        "dataset = dataset.fillna(dataset.mean())\n",
        "\n",
        "# (iii) Calculate Pearson correlation and plot heat map\n",
        "correlation_matrix = dataset.corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Pearson Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# Select columns having correlation <= 0.7\n",
        "correlation_threshold = 0.7\n",
        "columns_with_low_correlation = correlation_matrix.columns[correlation_matrix.max() <= correlation_threshold]\n",
        "print(\"Columns with correlation <= 0.7:\", columns_with_low_correlation.tolist())\n",
        "\n",
        "# (iv) Normalize values between 0 and 10\n",
        "dataset_normalized = (dataset - dataset.min()) / (dataset.max() - dataset.min()) * 10\n",
        "\n",
        "# (v) Replace values with 1 if <= 0.5 else with 0\n",
        "dataset_binary = dataset_normalized.applymap(lambda x: 1 if x <= 0.5 else 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAEJbzlpdm9O"
      },
      "source": [
        "Q14. Create a random dataset of 600 rows and 15 columns. All the values are defined between [-100,100].\n",
        "Perform the following operations:\n",
        "(i) Plot scatter graph between Column 5 and Column 6.\n",
        "(ii) Plot histogram of each column in single graph.\n",
        "(iii) Plot the Box plot of each column in single graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4aNGrD_dqDU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a random dataset\n",
        "dataset = pd.DataFrame(np.random.uniform(low=-100, high=100, size=(600, 15)), columns=[f'col_{i}' for i in range(1, 16)])\n",
        "\n",
        "# (i) Plot scatter graph between Column 5 and Column 6\n",
        "plt.scatter(dataset['col_5'], dataset['col_6'])\n",
        "plt.xlabel('Column 5')\n",
        "plt.ylabel('Column 6')\n",
        "plt.title('Scatter Plot: Column 5 vs Column 6')\n",
        "plt.show()\n",
        "\n",
        "# (ii) Plot histogram of each column in a single graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "for column in dataset.columns:\n",
        "    plt.hist(dataset[column], bins=20, alpha=0.5, label=column)\n",
        "plt.xlabel('Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Each Column')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# (iii) Plot the Box plot of each column in a single graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "dataset.boxplot()\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Box Plot of Each Column')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2OtxPsCd0w1"
      },
      "source": [
        "Q15. Create a random dataset of 500 rows and 5 columns:\n",
        "All the values are defined between [5,10].\n",
        "Perform the following operations:\n",
        "(i) Perform t-Test on each column.\n",
        "(ii) Perform Wilcoxon Signed Rank Test on each column.\n",
        "(iii) Perform Two Sample t-Test and Wilcoxon Rank Sum Test on Column 3 and Column 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLT-h5UFd5b8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_1samp, wilcoxon, ttest_ind, ranksums\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a random dataset\n",
        "dataset = pd.DataFrame(np.random.uniform(low=5, high=10, size=(500, 5)), columns=[f'col_{i}' for i in range(1, 6)])\n",
        "\n",
        "# (i) Perform t-Test on each column\n",
        "for column in dataset.columns:\n",
        "    t_stat, p_value = ttest_1samp(dataset[column], 7.5)  # Assuming population mean of 7.5\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"t-statistic: {t_stat:.4f}\")\n",
        "    print(f\"p-value: {p_value:.4f}\")\n",
        "    print()\n",
        "\n",
        "# (ii) Perform Wilcoxon Signed Rank Test on each column\n",
        "for column in dataset.columns:\n",
        "    w_stat, p_value = wilcoxon(dataset[column] - 7.5)  # Assuming population median of 7.5\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"Wilcoxon statistic: {w_stat:.4f}\")\n",
        "    print(f\"p-value: {p_value:.4f}\")\n",
        "    print()\n",
        "\n",
        "# (iii) Perform Two Sample t-Test and Wilcoxon Rank Sum Test on Column 3 and Column 4\n",
        "col_3 = dataset['col_3']\n",
        "col_4 = dataset['col_4']\n",
        "\n",
        "# Two Sample t-Test\n",
        "t_stat, p_value = ttest_ind(col_3, col_4)\n",
        "print(\"Two Sample t-Test - Column 3 vs Column 4\")\n",
        "print(f\"t-statistic: {t_stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "print()\n",
        "\n",
        "# Wilcoxon Rank Sum Test\n",
        "w_stat, p_value = ranksums(col_3, col_4)\n",
        "print(\"Wilcoxon Rank Sum Test - Column 3 vs Column 4\")\n",
        "print(f\"Wilcoxon statistic: {w_stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEaQtHHt1sBEgPgBqhD6Tv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}